---
output:
  md_document:
    variant: markdown_github
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

[![Travis-CI Build Status](https://travis-ci.org/hypertidy/vapour.svg?branch=master)](https://travis-ci.org/hypertidy/vapour)
[![AppVeyor Build Status](https://ci.appveyor.com/api/projects/status/github/hypertidy/vapour?branch=master&svg=true)](https://ci.appveyor.com/project/hypertidy/vapour)
[![Coverage Status](https://img.shields.io/codecov/c/github/hypertidy/vapour/master.svg)](https://codecov.io/github/hypertidy/vapour?branch=master)
[![CRAN_Status_Badge](http://www.r-pkg.org/badges/version/vapour)](https://cran.r-project.org/package=vapour)



```{r, echo = FALSE, include=FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "README-"
)
library(dplyr)
```

# vapour

The goal of vapour is to provide a basic **GDAL API** package for R. Ideally, this could become a common foundation for other packages to specialize. A parallel goal is to be freed from the powerful but sometimes limiting high-level data models of GDAL itself, specifically these are *simple features* and *affine-based regular rasters composed of 2D slices*. (GDAL will possibly remove these limitations over time but still there will always be value in having modularity in an ecosystem of tools. )

This is inspired by and draws heavily on work done [the sf package](https://github.com/r-spatial/sf) and rgdal and rgdal2. 
# Warning

There's a number of fragile areas in vapour, one in particular is the use of raster data sources that contain subdatasets - 
these are not handled, and they are not dealt with safely - if your source (NetCDF for example) contains subdatasets 
vapour currently will treat it like a raster and crash :)  Use at your own risk, this won't be fixed for a while ...



# Purpose

Current we have control to do the following: 

* read attributes only
* read geometry only
* read geometry as raw binary, or various text forms
* read geometry bounding box only
* (optionally) apply OGRSQL to a layer prior to any of the above http://www.gdal.org/ogr_sql.html

Limitations, work-in-progress and other discussion are active here: https://github.com/hypertidy/vapour/issues/4


## Examples

There's a function `vapour_read_attributes` that returns the attributes as  list of vectors. 

```{r}
pfile <- system.file("extdata", "point.shp", package = "vapour")
library(vapour)
vapour_read_attributes(pfile)
```

A higher level function *somewhere else* could wrap that function to return a data frame, but we don't want that in `vapour` because
it's not aligned with the goals of being lightweight and 
reducing the level of interpretation applied. The `data.frame` 
function in R is actually a very primitive implememtation for
data frames, so we avoid putting that interpretation on the data
and leave that up to the developer / user. 

```{r}
mvfile <- system.file("extdata/tab/list_locality_postcode_meander_valley.tab", package="vapour")

dat <- as.data.frame(vapour_read_attributes(mvfile),  stringsAsFactors = FALSE)

dim(dat)

head(dat)
```

## OGRSQL

Note that each lower-level function accepts a `sql` argument, 
which sends a query to the GDAL library to be executed against
the data source, this can create custom layers and so is independent of and ignores the `layer` argument. 

```{r}
vapour_read_attributes(mvfile, sql = "SELECT NAME, PLAN_REF FROM list_locality_postcode_meander_valley WHERE POSTCODE = 7310")


vapour_read_attributes(mvfile, sql = "SELECT NAME, PLAN_REF, FID FROM list_locality_postcode_meander_valley WHERE POSTCODE = 7306")


```

Also note that FID is a special row number value, to be used a as general facility for selecting by structural row. 

See http://www.gdal.org/ogr_sql.html

There are many useful higher level operations that can be used with this. The simplest is the ability to use GDAL as a database-like connection to attribute tables. 

A low-level function will return a character vector of JSON, GML, KML or WKT. 

```{r}
vapour_read_geometry(pfile)[5:6]  ## format = "WKB"

vapour_read_geometry_text(pfile)[5:6]  ## format = "json"

cfile <- system.file("extdata/sst_c.gpkg", package = "vapour")
vapour_read_geometry_text(pfile, format = "gml")[2]

## don't do this with a non-longlat data set like cfile
vapour_read_geometry_text(pfile, format = "kml")[1:2]

str(vapour_read_geometry_text(cfile, format = "wkt")[1:2])

```


We can combine these together to get a custom data set. 

```{r}
library(dplyr)
dat <- as.data.frame(vapour_read_attributes(cfile),  stringsAsFactors = FALSE) %>% dplyr::mutate(wkt = vapour_read_geometry_text(cfile, format = "wkt"))
glimpse(dat)
```

## Fast summary 

There is a basic function `vapour_read_extent` to return a straightforward bounding box vector for every feature, so that
we can flexibly build an index of a data set for later use. 

```{r}
mvfile <- system.file("extdata/tab/list_locality_postcode_meander_valley.tab", package="vapour")
str(vapour_read_extent(mvfile))

```

This makes for a very lightweight summary data set that will scale to hundreds of large inputs. 

```{r}
dat <- as.data.frame(vapour_read_attributes(mvfile), 
                     stringsAsFactors = FALSE)
library(raster)
dat$bbox <- vapour_read_extent(mvfile)

all_extent <- purrr::reduce(lapply(dat$bbox, raster::extent), raster::union)
##plot(all_extent)
##purrr::walk(lapply(dat$bbox, raster::extent), plot, add = TRUE)

```

An example is this set of *some number of* property boundary shapefiles, read into a few hundred Mb of simple features. 

```{r}
library(dplyr)
files <- raadfiles::thelist_files(format = "") %>% filter(grepl("parcel", fullname), grepl("shp$", fullname)) %>% 
  slice(1:8)
library(vapour)
system.time(purrr::map(files$fullname, sf::read_sf))
library(blob)

## our timing is competitive, and we get to choose what is read
## and when
read_table <- function(file) as.data.frame(vapour_read_attributes(file),  stringsAsFactors = FALSE)
system.time({
d <- purrr::map(files$fullname, read_table)
d <- dplyr::bind_rows(d)
g <- purrr::map(files$fullname, vapour_read_geometry)
d[["wkb"]] <- new_blob(unlist(g, recursive = FALSE))
})

```

We can read that in this simpler way for a quick data set to act as an index. 

```{r}
system.time({
  d <- purrr::map_df(files$fullname, read_table)
  d$bbox <- unlist(purrr::map(files$fullname, vapour_read_extent), recursive = FALSE)
})

pryr::object_size(d)
glimpse(d)

```


## Set up

I've kept a record of a minimal GDAL wrapper package here: 

https://github.com/mdsumner/gdalmin

This must be run when your function definitions change: 

```R
tools::package_native_routine_registration_skeleton("../vapour", "src/init.c",character_only = FALSE)
```

## Context

My first real attempt at DBI abstraction is here, this is still an aspect that is desperately needed in R to help bring tidyverse attention to spatial: 

https://github.com/mdsumner/RGDALSQL

Before that I had worked on getting sp and dplyr to at least work together https://github.com/dis-organization/sp_dplyrexpt and recently rgdal was updated to allow tibbles to be used, something that spbabel and spdplyr really needed to avoid friction. 

Early exploration of allow non-geometry read with rgdal was tried here: https://github.com/r-gris/gladr


Big thanks to Edzer Pebesma and Roger Bivand and Tim Keitt for prior art that I crib and copy from. Jeroen Ooms helped the R community hugely by providing an automatable build process for libraries on Windows. Mark Padgham helped kick me over a huge obstacle in using C++ libraries with R. Simon Wotherspoon and Ben Raymond have endured my ravings about wanting this level of control for many years.


# Code of conduct

Please note that this project is released with a [Contributor Code of Conduct](CONDUCT.md). By participating in this project you agree to abide by its terms.
